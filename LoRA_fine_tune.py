import torch
import json
from transformers import (
    AutoTokenizer,
    Gemma3ForCausalLM,
    TrainingArguments,
    Trainer,
    DataCollatorForLanguageModeling,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from datasets import Dataset
import os
from datetime import datetime
import time

# ì„¤ì •
MODEL_PATH = r"path/to/gemma-3-1b-it"
DATA_PATH = r"path/to/qa/dataset"
OUTPUT_DIR = r"path/to/analysis/results"

print("ğŸš€ BWR ì „ë¬¸ Gemma 1B LoRA íŒŒì¸íŠœë‹ ì‹œì‘!")
print(f"â° ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")


# ì§„í–‰ë„ í‘œì‹œ ì½œë°±
class ProgressCallback(TrainerCallback):
    def __init__(self):
        self.start_time = time.time()

    def on_log(self, args, state, control, logs=None, **kwargs):
        if logs and state.global_step > 0:
            current_step = state.global_step
            total_steps = state.max_steps
            elapsed = time.time() - self.start_time
            progress = (current_step / total_steps) * 100
            eta = (elapsed / current_step) * (total_steps - current_step)

            # ì§„í–‰ ë°”
            bar_length = 25
            filled_length = int(bar_length * progress / 100)
            bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)

            # í˜„ì¬ epoch ê³„ì‚°
            current_epoch = current_step / (total_steps / args.num_train_epochs)

            if current_step % 10 == 0:  # 10ìŠ¤í…ë§ˆë‹¤ ì¶œë ¥
                print(f"\nğŸ”„ [{bar}] {progress:.1f}% | "
                      f"Epoch: {current_epoch:.2f}/{args.num_train_epochs} | "
                      f"Step: {current_step}/{total_steps} | "
                      f"Loss: {logs.get('train_loss', 0):.4f} | "
                      f"ê²½ê³¼: {elapsed / 60:.0f}ë¶„ | ë‚¨ì€ì‹œê°„: {eta / 60:.0f}ë¶„")


# 1. Stratified ë°ì´í„° ë¡œë“œ
def load_stratified_data():
    """ë¬¸ì„œë³„ë¡œ ë¯¸ë¦¬ ë¶„í• ëœ train/eval ë°ì´í„° ë¡œë“œ"""

    train_filepath = os.path.join(DATA_PATH, "bwr_train_stratified.json")
    eval_filepath = os.path.join(DATA_PATH, "bwr_eval_stratified.json")

    # Train ë°ì´í„° ë¡œë“œ
    with open(train_filepath, 'r', encoding='utf-8') as f:
        train_data = json.load(f)
        train_qa_pairs = train_data['qa_pairs']

    # Eval ë°ì´í„° ë¡œë“œ
    with open(eval_filepath, 'r', encoding='utf-8') as f:
        eval_data = json.load(f)
        eval_qa_pairs = eval_data['qa_pairs']

    print(f"ğŸ“„ Train ë°ì´í„°: {len(train_qa_pairs)}ê°œ ë¡œë“œ")
    print(f"ğŸ“„ Eval ë°ì´í„°: {len(eval_qa_pairs)}ê°œ ë¡œë“œ")

    # ë¬¸ì„œë³„ ë¶„í¬ í™•ì¸
    train_sources = {}
    eval_sources = {}

    for qa in train_qa_pairs:
        source = qa.get('source', 'Unknown')
        train_sources[source] = train_sources.get(source, 0) + 1

    for qa in eval_qa_pairs:
        source = qa.get('source', 'Unknown')
        eval_sources[source] = eval_sources.get(source, 0) + 1

    print("ğŸ“Š Train ì„¸íŠ¸ ë¬¸ì„œë³„ ë¶„í¬:")
    for source, count in train_sources.items():
        print(f"  {source}: {count}ê°œ")

    print("ğŸ“Š Eval ì„¸íŠ¸ ë¬¸ì„œë³„ ë¶„í¬:")
    for source, count in eval_sources.items():
        print(f"  {source}: {count}ê°œ")

    return train_qa_pairs, eval_qa_pairs


# 2. ë°ì´í„° ì „ì²˜ë¦¬
def preprocess_data(train_qa_pairs, eval_qa_pairs, tokenizer):
    print("ğŸ”„ ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘...")

    # Train ë°ì´í„° ì „ì²˜ë¦¬
    train_texts = []
    for qa in train_qa_pairs:
        text = f"<bos><start_of_turn>user\n{qa['question']}<end_of_turn>\n<start_of_turn>model\n{qa['answer']}<end_of_turn><eos>"
        train_texts.append(text)

    train_tokenized = tokenizer(
        train_texts,
        truncation=True,
        padding=True,
        max_length=512,
        return_tensors="pt"
    )
    train_tokenized["labels"] = train_tokenized["input_ids"].clone()

    train_dataset = Dataset.from_dict({
        "input_ids": train_tokenized["input_ids"],
        "attention_mask": train_tokenized["attention_mask"],
        "labels": train_tokenized["labels"]
    })

    # Eval ë°ì´í„° ì „ì²˜ë¦¬
    eval_texts = []
    for qa in eval_qa_pairs:
        text = f"<bos><start_of_turn>user\n{qa['question']}<end_of_turn>\n<start_of_turn>model\n{qa['answer']}<end_of_turn><eos>"
        eval_texts.append(text)

    eval_tokenized = tokenizer(
        eval_texts,
        truncation=True,
        padding=True,
        max_length=512,
        return_tensors="pt"
    )
    eval_tokenized["labels"] = eval_tokenized["input_ids"].clone()

    eval_dataset = Dataset.from_dict({
        "input_ids": eval_tokenized["input_ids"],
        "attention_mask": eval_tokenized["attention_mask"],
        "labels": eval_tokenized["labels"]
    })

    print(f"âœ… í•™ìŠµ ë°ì´í„°: {len(train_dataset)}ê°œ")
    print(f"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(eval_dataset)}ê°œ")

    return {"train": train_dataset, "test": eval_dataset}


def main():
    print(f"ğŸ”§ CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    print(f"ğŸ”§ GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")

    print("ğŸ“¥ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
    model = Gemma3ForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        attn_implementation="eager",
        trust_remote_code=True,
        use_cache=False
    )

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id

    print(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: {model.num_parameters():,}")
    print(f"ğŸ“Š GPU ë©”ëª¨ë¦¬: {torch.cuda.memory_allocated(0) / 1024 ** 3:.2f}GB")

    # Stratified ë°ì´í„° ë¡œë“œ
    train_qa_pairs, eval_qa_pairs = load_stratified_data()
    dataset = preprocess_data(train_qa_pairs, eval_qa_pairs, tokenizer)

    model.train()

    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=[
            "q_proj", "k_proj", "v_proj", "o_proj",
            "gate_proj", "up_proj", "down_proj"
        ],
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )

    print("ğŸ”§ LoRA ì„¤ì • ì ìš© ì¤‘...")
    model.gradient_checkpointing_enable()
    model = prepare_model_for_kbit_training(model)
    model = get_peft_model(model, lora_config)

    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    all_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ¯ í•™ìŠµ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({trainable_params / all_params * 100:.2f}%)")

    training_args = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=2,
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=8,
        warmup_steps=50,
        learning_rate=2e-5,
        logging_steps=5,
        eval_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=200,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        greater_is_better=False,
        report_to=None,
        dataloader_num_workers=0,
        fp16=True,
        gradient_checkpointing=True,
        optim="adamw_torch",
        lr_scheduler_type="linear",
        remove_unused_columns=False,
    )

    total_steps = (len(dataset["train"]) // (
            training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps)) * training_args.num_train_epochs
    print(f"ğŸ“Š ì´ í•™ìŠµ ìŠ¤í…: {total_steps}")
    print(f"ğŸ’¡ ì˜ˆìƒ ì†Œìš” ì‹œê°„: {total_steps * 3 / 60:.0f}ë¶„")

    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False,
        return_tensors="pt"
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["test"],
        data_collator=data_collator,
        tokenizer=tokenizer,
        callbacks=[ProgressCallback()]
    )

    print("\nğŸš€ LoRA íŒŒì¸íŠœë‹ ì‹œì‘!")
    print("ğŸ“Š ì‹¤ì‹œê°„ ì§„í–‰ë„ê°€ í‘œì‹œë©ë‹ˆë‹¤...\n")

    try:
        trainer.train()

        print("\nğŸ’¾ ëª¨ë¸ ì €ì¥ ì¤‘...")
        trainer.save_model()
        tokenizer.save_pretrained(OUTPUT_DIR)

        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥: {OUTPUT_DIR}")
        print(f"â° ì™„ë£Œ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # í…ŒìŠ¤íŠ¸
        print("\nğŸ§ª í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸:")
        model.eval()

        test_prompts = [
            "What is a boiling water reactor?",
            "How do control rods work in BWR?"
        ]

        for prompt in test_prompts:
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,  # 50â†’100ìœ¼ë¡œ ì¦ê°€
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )

            response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
            print(f"\nQ: {prompt}")
            print(f"A: {response}")

    except Exception as e:
        print(f"\nâŒ í•™ìŠµ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
